{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extractor\n",
    "Inspiration for using Fasttext for grouping simmilar concepts and candidate generation: \n",
    "    https://aclweb.org/anthology/N18-2100\n",
    "\n",
    "First the train set is procressed to do assumptions. The actual evaluation will be done using text set at the end of this notebook. \n",
    "\n",
    "The Semeval 2010 is the dataset. It is available in the folder data. For evaluation, please unpack it to any location and make sure the correct locations are set in the cell below\n",
    "\n",
    "The extractor also needs Fasttext model, available here: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz . Please define path to this model below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAINSET_LOCATION = \"/tmp/keyword_gen/SemEval2010/train/\"\n",
    "TESTSET_LOCATION = \"/tmp/keyword_gen/SemEval2010/test/\"\n",
    "FASTTEXT_MODEL_PATH = \"/tmp/cc.en.300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import FastText\n",
    "\n",
    "cap_path = datapath(FASTTEXT_MODEL_PATH)\n",
    "fb_partial = FastText.load_fasttext_format(cap_path, full_model=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper class for discovery of relevant POS sequences\n",
    "acceptable_pos = set()\n",
    "acceptable_pos.add(\"J\")\n",
    "acceptable_pos.add(\"N\")\n",
    "#acceptable_pos.add(\"V\")\n",
    "\n",
    "class Trie():\n",
    "    def __init__(self, pos, parent=None, terminal = 0):\n",
    "        self.pos = pos\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.terminal = terminal\n",
    "        self.deph = 0\n",
    "        par = parent\n",
    "        while par is not None:\n",
    "            self.deph += 1\n",
    "            par = par.parent\n",
    "            \n",
    "    def add_node(self, pos):\n",
    "        if pos not in self.children:\n",
    "            self.children[pos] = Trie(pos, self)\n",
    "        return self.children[pos]\n",
    "        \n",
    "    def print_me(self, indent=\"\"):\n",
    "        print(indent+ self.pos+\": \")\n",
    "        for p, chld in self.children.items():\n",
    "            chld.print_me(indent+\"-\")\n",
    "    \n",
    "    def can_start(self, pos):\n",
    "        if len(pos)>0 and pos[0] in acceptable_pos and pos[0] in self.children:\n",
    "            return self.children[pos[0]]\n",
    "        return self  \n",
    "    \n",
    "    def can_move(self, pos):\n",
    "        if len(pos)>0 and pos[0] in acceptable_pos and pos[0] in self.children:\n",
    "            return True\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset: Semeval\n",
    "Semeval is used for results comparison with Key2Vec paper (mentioned in the beginning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"textcat\"])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "class dataset_loader():\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.documents = {}\n",
    "        self.keywords = {}\n",
    "        self.keywords_count = 0\n",
    "        \n",
    "    def get_all_files(self, extension):\n",
    "        all_files = []\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith(extension):\n",
    "                     all_files.append(os.path.join(root, file))\n",
    "        return all_files\n",
    "    \n",
    "    def read_file(self, path):\n",
    "        lines = []\n",
    "        with open(path) as fp:  \n",
    "            lines = fp.readlines()\n",
    "        return lines\n",
    "    \n",
    "    # extract seuences word words which POS tags have the same sequence as keywords pos sequences\n",
    "    def extract_keywords_candidates(self, root, minimal_frequency=2, stemmer=None):\n",
    "        candidates = {}\n",
    "        doc_candidates = {}\n",
    "        document_embeddings = {}\n",
    "        for doc_id, text in self.abstracts.items():\n",
    "            doc_embedding = None\n",
    "            print(doc_id+\", \", end = '')\n",
    "            if doc_id not in doc_candidates:\n",
    "                doc_candidates[doc_id]={}\n",
    "            processed = nlp(text)\n",
    "            sent_id = 0\n",
    "            for sent in processed.sents:\n",
    "                sent_id += 1\n",
    "                sent_toks = []\n",
    "                tmp_currents = [root]\n",
    "                tmp_currents2 = []\n",
    "                local_candidates = []\n",
    "                for token in sent:\n",
    "                    if token.is_alpha and not token.is_stop and len(token.lemma_)>3: # keywords are mostly alfa and dont contain stop words\n",
    "                        if stemmer is None:\n",
    "                            sent_toks.append(token.lemma_)\n",
    "                        else:\n",
    "                            sent_toks.append(stemmer.stem(token.text))\n",
    "                    else:\n",
    "                        sent_toks.append(\"--ommit--\")\n",
    "                    # print(token.lemma_+\"  len curremt \"+str(len(tmp_currents)))\n",
    "                    for current in tmp_currents:\n",
    "                        is_valid_keyword = True\n",
    "                        new_current = current.can_start(token.tag_)\n",
    "                        if current == new_current: #fail or start from scratch \n",
    "                            current = root\n",
    "                            \n",
    "                        else: # move forward\n",
    "                            current = new_current\n",
    "                        if current.terminal > minimal_frequency:\n",
    "                            #avoid non words\n",
    "                            for tok in sent_toks[-current.deph:]:\n",
    "                                if tok==\"--ommit--\":\n",
    "                                    is_valid_keyword = False\n",
    "                            candidate_sent = \" \".join(sent_toks[-current.deph:])\n",
    "                            #avoid repetitions\n",
    "                            if is_valid_keyword:\n",
    "                                for lc in local_candidates:\n",
    "                                    lcstr = \" \".join(lc)\n",
    "                                    if candidate_sent == lcstr:\n",
    "                                        is_valid_keyword = False\n",
    "                                        break\n",
    "                            if is_valid_keyword:\n",
    "                                local_candidates.append(sent_toks[-current.deph:])\n",
    "                                #print(\"Add to local candidates \"+\" \".join(sent_toks[-current.deph:])+\"   \"+str(current.deph) )\n",
    "                            \n",
    "                        if current != root and is_valid_keyword:\n",
    "                            tmp_currents2.append(current)\n",
    "                            potential_start = root.can_start(token.tag_)\n",
    "                            if root!=potential_start and current!= potential_start:\n",
    "                                tmp_currents2.append(potential_start)\n",
    "                    tmp_currents = tmp_currents2.copy()\n",
    "                    if len(tmp_currents)==0:\n",
    "                        if len(local_candidates)>0:# select best from local candidates\n",
    "                            longest = None\n",
    "                            for lc in local_candidates:\n",
    "                                if longest is None or len(lc) > len(longest):\n",
    "                                    longest=lc\n",
    "                            candidate_str = \" \".join(longest)\n",
    "                            if candidate_str not in candidates:\n",
    "                                candidates[candidate_str]= {}\n",
    "                            if doc_id not in candidates[candidate_str]:\n",
    "                                candidates[candidate_str][doc_id]=0\n",
    "                            candidates[candidate_str][doc_id]+=1\n",
    "                            if candidate_str not in doc_candidates[doc_id]:\n",
    "                                doc_candidates[doc_id][candidate_str]=0\n",
    "                            doc_candidates[doc_id][candidate_str]+=1\n",
    "                            if sent_id <= 10:\n",
    "                                if doc_embedding is None:\n",
    "                                    doc_embedding = np.array(fb_partial.wv[candidate_str])\n",
    "                                else:\n",
    "                                    doc_embedding+=np.array(fb_partial.wv[candidate_str])\n",
    "                        local_candidates.clear()\n",
    "                        tmp_currents = [root]\n",
    "                    tmp_currents2.clear()   \n",
    "            document_embeddings[doc_id] = doc_embedding\n",
    "        return candidates, doc_candidates, document_embeddings\n",
    "    \n",
    "class semelval_loader(dataset_loader):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        super().__init__(path)\n",
    "        \n",
    "        self.keywords_reader = {}\n",
    "        self.keywords_combined = {}\n",
    "        self.abstracts = {}\n",
    "        self.keywords_combined_count =0\n",
    "        self.keywords_reader_count =0\n",
    "        \n",
    "    def _get_abstract(self, lines, all_document=False):\n",
    "        ret = \"\"\n",
    "        i = 0\n",
    "        start = False\n",
    "        for l in lines:\n",
    "            l=l.strip()\n",
    "            if not all_document and l == \"1. INTRODUCTION\":\n",
    "                break\n",
    "            if \". REFERENCES\" in l:\n",
    "                break\n",
    "            if i==0:\n",
    "                ret = l\n",
    "            elif i == 1 or start:\n",
    "                ret += \" \"+l  \n",
    "            if l == \"ABSTRACT\":\n",
    "                start = True\n",
    "            i+=1\n",
    "        return ret.replace(\"-\", \" \")\n",
    "    \n",
    "    def read_keywords(self, content):\n",
    "        ret_keywords = {}\n",
    "        total_keywords = 0\n",
    "        for c in content:\n",
    "            splitted = c.split(\":\")\n",
    "            doc_id = splitted[0].strip()\n",
    "            keywords = splitted[1].strip().replace(\"-\", \" \").split(\",\")\n",
    "            ret_keywords[doc_id]=keywords\n",
    "            total_keywords += len(keywords)\n",
    "        return ret_keywords, total_keywords\n",
    "    \n",
    "    def load_data(self, side=\"train\"):\n",
    "        files = self.get_all_files(\"final\")\n",
    "        stem = \"\"\n",
    "        if side==\"test\":\n",
    "            stem=\"stem.\"\n",
    "        for f in files:\n",
    "            fname=f.split(\"/\")[-1]\n",
    "            if \"txt.final\" in fname:\n",
    "                content = self.read_file(f)\n",
    "                self.abstracts[fname[:4]]=self._get_abstract(content)\n",
    "                self.documents[fname[:4]]=self._get_abstract(content, True)\n",
    "            elif fname==side+\".author.\"+stem+\"final\":\n",
    "                content = self.read_file(f)\n",
    "                self.keywords, self.keywords_count = self.read_keywords(content)\n",
    "            elif fname==side+\".combined.\"+stem+\"final\":\n",
    "                content = self.read_file(f)\n",
    "                self.keywords_combined, self.keywords_combined_count = self.read_keywords(content)\n",
    "            elif fname==side+\".reader.\"+stem+\"final\":\n",
    "                content = self.read_file(f)\n",
    "                self.keywords_reader, self.keywords_reader_count = self.read_keywords(content)\n",
    "                  \n",
    "ds = semelval_loader(TRAINSET_LOCATION)\n",
    "ds.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What keywords are better to use?\n",
    "Posisble options are: authors, readers and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Abstracts--\n",
      "Authors keyword coverage 220 559 = 0.3935599284436494\n",
      "Readers keyword coverage 686 1824 = 0.37609649122807015\n",
      "Combined keyword coverage 823 2223 = 0.3702204228520018\n",
      "--Full documents--\n",
      "Authors keyword coverage 367 559 = 0.6565295169946332\n",
      "Readers keyword coverage 1429 1824 = 0.7834429824561403\n",
      "Combined keyword coverage 1672 2223 = 0.7521367521367521 \n",
      "\n",
      "Average keyword count for author 3.8819444444444446\n",
      "Average keyword count for reader 12.666666666666666\n",
      "Average keyword count combined 15.4375\n"
     ]
    }
   ],
   "source": [
    "#compare keywords coverage\n",
    "\n",
    "def keyword_coverage(documents, keywords_corpora):\n",
    "    all_keywords = 0\n",
    "    covered_keywords = 0\n",
    "    for doc_id, keywords in keywords_corpora.items():\n",
    "        text  = documents[doc_id]\n",
    "        all_keywords += len(keywords)\n",
    "        for keyword in keywords:\n",
    "            if keyword[:-1] in text: #simplified stemming just for stat purposes\n",
    "                covered_keywords += 1\n",
    "    return covered_keywords, all_keywords\n",
    "\n",
    "print(\"--Abstracts--\")\n",
    "n , a = keyword_coverage(ds.abstracts, ds.keywords)\n",
    "print(\"Authors keyword coverage \"+str(n)+\" \"+str(a)+\" = \"+str(n/a))\n",
    "\n",
    "n , a = keyword_coverage(ds.abstracts, ds.keywords_reader)\n",
    "print(\"Readers keyword coverage \"+str(n)+\" \"+str(a)+\" = \"+str(n/a))\n",
    "\n",
    "n , a = keyword_coverage(ds.abstracts, ds.keywords_combined)\n",
    "print(\"Combined keyword coverage \"+str(n)+\" \"+str(a)+\" = \"+str(n/a))\n",
    "\n",
    "print(\"--Full documents--\")\n",
    "n , a = keyword_coverage(ds.documents, ds.keywords)\n",
    "print(\"Authors keyword coverage \"+str(n)+\" \"+str(a)+\" = \"+str(n/a))\n",
    "\n",
    "n , a = keyword_coverage(ds.documents, ds.keywords_reader)\n",
    "print(\"Readers keyword coverage \"+str(n)+\" \"+str(a)+\" = \"+str(n/a))\n",
    "\n",
    "n , a = keyword_coverage(ds.documents, ds.keywords_combined)\n",
    "print(\"Combined keyword coverage \"+str(n)+\" \"+str(a)+\" = \"+str(n/a)+\" \\n\")\n",
    "\n",
    "print(\"Average keyword count for author \"+str(ds.keywords_count / len(ds.keywords) ) )\n",
    "print(\"Average keyword count for reader \"+str(ds.keywords_reader_count / len(ds.keywords_reader) ) )\n",
    "print(\"Average keyword count combined \"+str(ds.keywords_combined_count / len(ds.keywords_combined) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For further processing we'll use combined keywords as they contain largest number of words, with only slightly lower coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.keywords = ds.keywords_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus on combined readers and authors keywords are they are most popular with slightly smaller coverage than readers' keywords\n",
    "\n",
    "## How do keywords look like?\n",
    "What are most common POS sequences among them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular POSes among keywords [('SYM', 1), ('ADD', 1), ('.', 1), ('``', 1), ('VBZ', 1), ('RP', 1), ('POS', 2), ('JJS', 2), ('AFX', 2), ('NNP', 3), ('LS', 4), ('JJR', 4), ('XX', 5), ('DT', 6), ('UH', 8), ('TO', 10), ('CD', 11), ('FW', 12), ('VBD', 20), ('RB', 20), ('NNS', 31), ('CC', 38), ('VBP', 41), ('VBG', 89), ('VB', 101), ('IN', 102), ('VBN', 162), ('JJ', 868), ('NN', 3646)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#what kind of POS'es are the keywords.\n",
    "\n",
    "pos_frequency = {}\n",
    "keywors_sequence_freqs = {}\n",
    "keywords_freq = {}\n",
    "root = Trie(\"\")\n",
    "for doc_id, keywords in ds.keywords_combined.items():\n",
    "    for keyword in keywords:\n",
    "        if keyword not in keywords_freq:\n",
    "            keywords_freq[keyword] = 0\n",
    "        keywords_freq[keyword] += 1\n",
    "        \n",
    "        processed = nlp(keyword)\n",
    "        sequence = \"\"\n",
    "        nnode = root\n",
    "        for token in processed:\n",
    "            nnode = nnode.add_node(token.tag_[0])\n",
    "#             print(token.text, token.lemma_, token.pos_, token.tag_,\n",
    "#                     token.shape_, token.is_alpha, token.is_stop)\n",
    "            sequence+=token.tag_+\" \"\n",
    "            if token.tag_ not in pos_frequency:\n",
    "                pos_frequency[token.tag_] = 0\n",
    "            pos_frequency[token.tag_] += 1\n",
    "        nnode.terminal += 1\n",
    "        if sequence not in keywors_sequence_freqs:\n",
    "            keywors_sequence_freqs[sequence] = 0\n",
    "        keywors_sequence_freqs[sequence] += 1\n",
    "pos_frequency = sorted(pos_frequency.items(), key=operator.itemgetter(1))\n",
    "keywors_sequence_freqs = sorted(keywors_sequence_freqs.items(), key=operator.itemgetter(1))\n",
    "keywords_freq = sorted(keywords_freq.items(), key=operator.itemgetter(1))\n",
    "            \n",
    "print(\"Most popular POSes among keywords \"+str(pos_frequency))\n",
    "# print(\"Keywords POS sequences \"+str(keywors_sequence_freqs))\n",
    "# print(\"How uniformly keywords are spreaded across documents \"+str(keywords_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Keyword candidates will be havying the structure of the target keywords, limitted to POS starting with *N* , *J* and *V* to cover most popular keyword structures\n",
    "\n",
    "# Extract keyword candidates\n",
    "Process documents and extract token sequences which reflect keywords POS sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents:\n",
      "C-57, H-52, I-66, I-51, H-44, J-40, I-45, J-50, C-45, H-48, H-50, I-38, I-37, I-54, H-41, J-67, J-51, H-92, H-38, H-53, H-81, J-36, H-88, C-77, H-84, H-62, C-46, J-44, C-44, H-73, J-47, H-96, I-58, C-75, I-73, C-52, I-60, J-37, I-47, J-55, I-59, H-69, H-79, J-66, J-45, C-56, H-64, J-42, I-57, H-47, I-43, I-55, I-65, J-73, C-55, C-81, H-49, C-61, J-62, J-38, C-50, C-78, H-46, J-70, I-77, J-59, I-75, I-64, J-71, I-42, C-62, I-56, H-97, J-49, J-39, C-74, C-67, H-45, J-61, J-56, I-48, J-74, H-85, C-71, C-72, H-54, H-35, H-87, C-80, C-69, I-74, H-98, J-60, C-66, I-49, H-60, C-54, I-53, J-58, C-76, I-68, C-48, J-57, J-34, J-53, I-62, I-76, I-46, C-41, C-83, J-41, C-42, J-69, I-71, C-58, I-52, H-43, H-90, J-33, J-72, J-65, H-61, H-83, I-61, J-63, J-52, I-50, H-42, C-65, C-49, H-77, C-68, H-37, I-63, H-82, C-53, H-63, I-70, H-40, I-72, H-95, J-35, C-79, C-84, "
     ]
    }
   ],
   "source": [
    "print(\"Processed documents:\")\n",
    "keywords, doc_keywords, doc_embeddings = ds.extract_keywords_candidates(root, minimal_frequency=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather embeddings\n",
    "For all candidate keywords, gether their embeddings to avoid repetitive processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_embeddings(keywords):\n",
    "    embeddings = {}\n",
    "    for word, _ in keywords.items():\n",
    "        embeddings[word] = fb_partial.wv[word]\n",
    "    return embeddings\n",
    "\n",
    "embeddings = gather_embeddings(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate filtering\n",
    "To select best keyword candidates for each document, we'll use tf-idf. \n",
    "\n",
    "Since the target keywords often express same meanings with different words (for e.g. load-dependent resource failure, load-dependent failure), the tf-idf will be calculated in a fuzzy way for meanings instead of words. \n",
    "\n",
    "To group words into meanings we'll use simple threashold, assessed by experiemnts on trainset\n",
    "\n",
    "The tf for a given word will be calculated by adding to the usual tf, the frequencies of the words having the same meaning, multiplied by the cosine simmilarity of their embeddings.\n",
    "\n",
    "Idf is calculated in simmilar manner - by adding to usual idf, numbers of documents the \"sibling\" word appeared in, multiplied by their simmilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import itertools\n",
    "\n",
    "PROCESS_COUNT = cpu_count()-1\n",
    "simmilarity_threshold = 0.8 # assumption derived from manual simmilarity analysis \n",
    "\n",
    "def group_results(scores, groups):\n",
    "    result = {}\n",
    "    for word, score in scores:\n",
    "        group_processed = False\n",
    "        if word in groups:\n",
    "            for w in groups[word]:\n",
    "                if w in result:\n",
    "                    group_processed = True\n",
    "                    break\n",
    "            if group_processed:\n",
    "                continue\n",
    "        result[word] = score\n",
    "    return result\n",
    "            \n",
    "\n",
    "def calculate_tf(keywords, embeddings, simmilarity_threshold):\n",
    "    tf_scores = {}\n",
    "    groups = {}\n",
    "    \n",
    "    all_words = list(keywords.keys())\n",
    "    word_embeddings=[]\n",
    "    for word2, freq in keywords.items():\n",
    "        word_embeddings.append(embeddings[word2])\n",
    "    for word, freq in keywords.items():\n",
    "        word_embedding = embeddings[word]\n",
    "        dists = []\n",
    "        \n",
    "        dists = fb_partial.wv.cosine_similarities(word_embedding, word_embeddings)\n",
    "        score = 0\n",
    "        for d, w in zip(dists, all_words):\n",
    "            if d > simmilarity_threshold:\n",
    "                score += d * keywords[w]\n",
    "                if w != word:\n",
    "                    if word not in groups and w not in groups:\n",
    "                        groups[word] = set()\n",
    "                        groups[w] = groups[word]\n",
    "                    elif word not in groups:\n",
    "                        groups[word] = groups[w]\n",
    "                    else:\n",
    "                        groups[w] = groups[word]\n",
    "                    groups[word].add(w)\n",
    "                    groups[word].add(word)\n",
    "                    groups[w].add(w)\n",
    "                    groups[w].add(word)\n",
    "                #print(word+\"   <-  \"+w+\"  \"+str(d)+\"  \"+str(keywords[w]))\n",
    "        tf_scores[word] = score\n",
    "    tf_scores = sorted(tf_scores.items(), key=lambda tup: tup[1], reverse=True)\n",
    "    tf_scores = group_results(tf_scores, groups)\n",
    "    return tf_scores, groups\n",
    "\n",
    "#modified idf ?- calculates in how many documents a word appeared with what strenghts\n",
    "def idf(word, embeddings, keywords, doclen, simmilarity_threshold, groups):\n",
    "    best_score = 0\n",
    "    best_group_word = \"\"\n",
    "    if word not in groups:\n",
    "        groups[word] = set()\n",
    "        groups[word].add(word)\n",
    "    for wg in groups[word]:\n",
    "        doc_number = len(keywords[wg]) -1\n",
    "#         for w, _ in keywords.items():\n",
    "#             dist = fb_partial.wv.cosine_similarities(embeddings[wg], [embeddings[w]])\n",
    "#             if dist > simmilarity_threshold:\n",
    "#                 doc_number += dist * (len(keywords[w]) -1 )\n",
    "        score = math.log(doclen / (doc_number + 0.1))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_group_word = wg\n",
    "    return best_score, best_group_word\n",
    "\n",
    "def tfidf(tf_keywords, doc_embedding, keywords, embeddings, doclen, simmilarity_threshold, groups):\n",
    "    result = []\n",
    "    for word, tf in tf_keywords.items():\n",
    "        idf_score, best_word = idf(word, embeddings, keywords, doclen, simmilarity_threshold, groups)\n",
    "        cos_simmilar_to_doc = fb_partial.wv.cosine_similarities(doc_embedding, [embeddings[word]])[0]\n",
    "        #print(word+\" -> \"+best_word+\"   tf= \"+str(tf)+\"   idf =  \"+str(idf_score)+\"  cos_simmilar_to_doc= \"+str(cos_simmilar_to_doc) )\n",
    "        result.append( (word, tf * idf_score *cos_simmilar_to_doc ) ) # tf-idf adjusted by simmilarity to document embedding\n",
    "    result = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "    return result\n",
    "\n",
    "def predict(doc_id, doc_words,doc_embedding, keywords, embeddings, simmilarity_threshold):\n",
    "    predictions = {}\n",
    "    groups_dict = {}\n",
    "    #print(\"start \"+doc_id)\n",
    "    tf_keywords, groups = calculate_tf(doc_words, embeddings, simmilarity_threshold)\n",
    "    keyword_prediction = tfidf(tf_keywords, doc_embedding, keywords, embeddings, len(doc_keywords), simmilarity_threshold, groups)\n",
    "    predictions[doc_id] = keyword_prediction[:15] # 15 keywords are the Semeval requirements\n",
    "    groups_dict[doc_id] = groups\n",
    "    #print(\"end \"+doc_id+\"  -> \")#\n",
    "    return predictions, groups_dict\n",
    "\n",
    "docs = list(doc_keywords.keys())\n",
    "keyword_lists = []\n",
    "embeddings_list = []\n",
    "for d in docs:\n",
    "    keyword_lists.append(doc_keywords[d])\n",
    "    embeddings_list.append(doc_embeddings[d])\n",
    "pool = Pool(processes=PROCESS_COUNT)\n",
    "results = pool.starmap(predict, zip(docs, keyword_lists,embeddings_list, itertools.repeat(keywords), itertools.repeat(embeddings),itertools.repeat(simmilarity_threshold), ) )\n",
    "groups_dict={}\n",
    "predictions={}\n",
    "for p,g in results:\n",
    "    for docid, vals in p.items():\n",
    "        predictions[docid] = vals\n",
    "    for docid, vals in g.items():\n",
    "        groups_dict[docid] = vals\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.15831381733021077   Recall = 0.1525959367945824\n"
     ]
    }
   ],
   "source": [
    "def calculate_stats(predictions, groups_dict, gold_keywords, use_meanings=False, show_details = False):\n",
    "    tp =0\n",
    "    fp=0\n",
    "    tn=0\n",
    "    fn=0\n",
    "    for doc_id, doc_gold_keywords in gold_keywords.items():\n",
    "        if doc_id not in predictions:\n",
    "            continue\n",
    "        if show_details:\n",
    "            print(\"Detailed results for document \"+doc_id)\n",
    "        preds = predictions[doc_id]\n",
    "        groups = groups_dict[doc_id]\n",
    "        if show_details and use_meanings:\n",
    "            print(\"Stats are calculated based on meaning groups:\")\n",
    "            i=0\n",
    "            shown = set()\n",
    "            for word, st in groups.items():\n",
    "                if \"\".join(st) in shown or len(st)<=1:\n",
    "                    continue\n",
    "                print(\"Meaning group \"+str(i)+\" = \"+str(st))\n",
    "                shown.add(\"\".join(st))\n",
    "                i+=1\n",
    "        #precision - related\n",
    "        pred_words_set = set()\n",
    "        for pred_word, score in preds:\n",
    "            if use_meanings:\n",
    "                if pred_word not in groups:\n",
    "                    groups[pred_word] = set(list(pred_word))\n",
    "                found = False\n",
    "                for gword in groups[pred_word]:\n",
    "                    if gword in doc_gold_keywords:\n",
    "                        tp+=1\n",
    "                        found = True\n",
    "                        if show_details:\n",
    "                            print(\"Correctly predicted meaning: \"+str(groups[pred_word])+\" score= \"+str(score))\n",
    "                        break\n",
    "                if not found:\n",
    "                    fp += 1\n",
    "                pred_words_set |= groups[pred_word]\n",
    "            else:\n",
    "                if pred_word in doc_gold_keywords:\n",
    "                    if show_details:\n",
    "                        print(\"Correctly predicted keyword: \"+pred_word+\" score= \"+str(score))\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    fp+=1\n",
    "                pred_words_set.add(pred_word)\n",
    "        #recall related\n",
    "        for gold_word in doc_gold_keywords:\n",
    "            if gold_word not in pred_words_set:\n",
    "                fn+=1\n",
    "    if show_details:\n",
    "        print(\"True positives: \"+str(tp))\n",
    "        print(\"False positives: \"+str(fp))\n",
    "        print(\"False negatives: \"+str(fn))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print(\"Precision = \"+str(precision)+\"   Recall = \"+str(recall))\n",
    "calculate_stats(predictions, groups_dict, ds.keywords_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above statistics assumes simple word matching.\n",
    "\n",
    "If we aim for meaning matching (as initially assumed), the results will be following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.17892271662763465   Recall = 0.1734786557674841\n"
     ]
    }
   ],
   "source": [
    "calculate_stats(predictions, groups_dict, ds.keywords_combined, use_meanings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning groups are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-57 - meaning group 0 = {'congestion setting', 'congestion games', 'congestion game'}\n",
      "C-57 - meaning group 1 = {'loaddependent failure', 'load dependent failures', 'load dependent failure'}\n",
      "C-57 - meaning group 2 = {'identical resource', 'identical resources'}\n",
      "C-57 - meaning group 3 = {'distributed artificial intelligence', 'artificial intelligence'}\n",
      "H-52 - meaning group 0 = {'vocabulary independent spoken term', 'vocabulary independent system'}\n",
      "H-52 - meaning group 1 = {'phonetic lattice', 'phonetic transcript', 'such phonetic transcript'}\n",
      "H-52 - meaning group 2 = {'information search', 'information storage'}\n",
      "I-66 - meaning group 0 = {'global optimality', 'global optimal algorithm'}\n",
      "I-51 - meaning group 0 = {'argumentation framework', 'counterargument generation policy', 'argument generation policy', 'argumentation process'}\n",
      "I-51 - meaning group 1 = {'distributed artificial intelligence multiagent', 'artificial intelligence'}\n",
      "I-45 - meaning group 0 = {'programming languages', 'programming language'}\n",
      "I-45 - meaning group 1 = {'commitment machine', 'commitment machine framework'}\n",
      "J-50 - meaning group 0 = {'common voting rule', 'common voting'}\n",
      "H-48 - meaning group 0 = {'query expansion technique', 'query expansion', 'evaluating query expansion'}\n",
      "H-48 - meaning group 1 = {'term mismatch', 'query document term mismatch'}\n",
      "H-48 - meaning group 2 = {'information search', 'information storage'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    i=0\n",
    "    shown = set()\n",
    "    for word, st in groups_dict[d].items():\n",
    "        if \"\".join(st) in shown or len(st)<=1:\n",
    "            continue\n",
    "        print(d+\" - meaning group \"+str(i)+\" = \"+str(st))\n",
    "        shown.add(\"\".join(st))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using testset\n",
    "Warning! Before test evaluation ,please make sure the answer files are copied to test directory (where the documents are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-6., C-17, J-14, J-7., J-4., H-17, I-35, C-19, I-20, C-28, J-21, I-31, I-6., I-32, I-26, H-30, J-22, H-16, H-32, C-20, I-34, H-2., H-26, J-9., J-27, H-13, C-34, C-8., I-33, J-11, I-29, J-26, H-9., C-29, C-9., I-19, J-2., I-21, H-14, I-15, I-14, C-4., J-1., I-1., H-12, I-9., I-11, J-10, H-20, I-5., C-27, I-16, H-10, I-18, H-29, J-31, H-4., H-19, C-33, C-23, I-7., C-14, H-24, I-12, H-31, I-30, H-21, C-1., H-7., J-32, J-3., J-18, I-4., C-18, J-30, J-28, C-22, C-3., H-11, J-17, H-25, H-3., C-36, C-38, C-86, H-8., J-15, J-23, H-5., I-22, J-8., C-30, J-20, J-25, J-13, I-10, C-31, C-32, C-40, H-18, "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ts = semelval_loader(TESTSET_LOCATION)\n",
    "ts.load_data(side=\"test\")\n",
    "ts.keywords = ts.keywords_combined\n",
    "tkeywords, tdoc_keywords, tdoc_embeddings = ts.extract_keywords_candidates(root, stemmer=ps)\n",
    "tembeddings = gather_embeddings(tkeywords)\n",
    "\n",
    "docs = list(tdoc_keywords.keys())\n",
    "keyword_lists = []\n",
    "embeddings_list = []\n",
    "for d in docs:\n",
    "    keyword_lists.append(tdoc_keywords[d])\n",
    "    embeddings_list.append(tdoc_embeddings[d])\n",
    "pool = Pool(processes=PROCESS_COUNT)\n",
    "results = pool.starmap(predict, zip(docs, keyword_lists,embeddings_list, itertools.repeat(tkeywords), itertools.repeat(tembeddings),itertools.repeat(simmilarity_threshold), ) )\n",
    "groups_dict={}\n",
    "predictions={}\n",
    "for p,g in results:\n",
    "    for docid, vals in p.items():\n",
    "        predictions[docid] = vals\n",
    "    for docid, vals in g.items():\n",
    "        groups_dict[docid] = vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword stats: \n",
      "Precision = 0.17477477477477477   Recall = 0.18780251694094868\n",
      "meaning stats: \n",
      "Precision = 0.17477477477477477   Recall = 0.18780251694094868\n"
     ]
    }
   ],
   "source": [
    "print(\"keyword stats: \")\n",
    "calculate_stats(predictions, groups_dict, ts.keywords_combined, True)\n",
    "print(\"meaning stats: \")\n",
    "calculate_stats(predictions, groups_dict, ts.keywords_combined, use_meanings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work\n",
    "\n",
    "Adapt Key2Vec pagerank for adjusting kaywords ranking\n",
    "\n",
    "Search for optimal hyperparameters (simmilarity_threshold, pos sequence frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results supporting assumptions\n",
    "\n",
    "### What pos tags to allow to be part of keywords\n",
    "\n",
    "set of 10 with N J V   \n",
    "without meaning groups: Precision = 0.2318840579710145   Recall = 0.21768707482993196  \n",
    "with meaning groups: Precision = 0.2391304347826087   Recall = 0.22602739726027396  \n",
    "\n",
    "set of 10 with N J V C   \n",
    "without meaning groups: Precision = 0.21739130434782608   Recall = 0.20408163265306123    \n",
    "with meaning groups: Precision = 0.2246376811594203   Recall = 0.21232876712328766   \n",
    "\n",
    "set of 10 with N J  \n",
    "without meaning groups: Precision = 0.2608695652173913   Recall = 0.24489795918367346  \n",
    "with meaning groups: Precision = 0.26811594202898553   Recall = 0.2534246575342466  \n",
    "\n",
    "#### NJ give best performance  \n",
    "#### Meaning groups improve performance\n",
    "\n",
    "### Process whole documents of just abstracts?\n",
    "\n",
    "all set for full documents:  \n",
    "Precision = 0.10896309314586995   Recall = 0.13982859720342805  \n",
    "Precision = 0.1539543057996485   Recall = 0.20249653259361997  \n",
    "\n",
    "all set for full abstracts:  \n",
    "Precision = 0.14894613583138172   Recall = 0.1435665914221219  \n",
    "Precision = 0.17283372365339578   Recall = 0.16857012334399268  \n",
    "\n",
    "#### Processing abstracts give better results\n",
    "\n",
    "### Doest fuzzy idf help?\n",
    "\n",
    "set of 20 abstracts with Fuzzy IDF  \n",
    "Precision = 0.21180555555555555   Recall = 0.20819112627986347  \n",
    "\n",
    "set of 20 abstracts without Fuzzy IDF  \n",
    "Precision = 0.21875   Recall = 0.2150170648464164  \n",
    "\n",
    "#### Fuzzy IDF is a bad idea\n",
    "\n",
    "### Do deocument embeddings help in addition to tf-idf\n",
    "\n",
    "test set with document embeddings\n",
    "Precision = 0.18018018018018017   Recall = 0.1937984496124031\n",
    "\n",
    "test set without document embeddings\n",
    "Precision = 0.17477477477477477   Recall = 0.18780251694094868\n",
    "\n",
    "#### Using document embeddings to compare keyword candidate embeddings and use them for ranking helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
